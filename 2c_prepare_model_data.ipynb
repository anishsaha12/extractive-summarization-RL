{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "lSe7nuS9SHHI"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import torch\n",
    "import pickle\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2YeGUkbxSpfx"
   },
   "outputs": [],
   "source": [
    "data_base = 'data'\n",
    "dataset_name = 'cnn_dailymail'\n",
    "\n",
    "data_path = data_base+'/'+dataset_name\n",
    "sentence_emb_path = data_path+'/sentence_embs'\n",
    "top_sentence_emb_path = data_path+'/top_sentence_embs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_from_disk(\n",
    "#     'data/cnn_dailymail/sentences/test/test_rewards'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_from_disk(\n",
    "#     'data/cnn_dailymail/sentences/validation/val_rewards'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk(\n",
    "    'data/cnn_dailymail/sentences/train/train_rewards'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kept_article_ids_chunkl(split, chunk_num):\n",
    "    with open(\n",
    "          top_sentence_emb_path+'/'+split+'/'+split+'_data_article_ids_part_'+str(chunk_num)+\".pkl\", \n",
    "          \"rb\"\n",
    "        ) as input_f:\n",
    "            keep_a_ids = pickle.load(input_f)\n",
    "    return keep_a_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_a_ids = [get_kept_article_ids_chunkl('train',p) for p in [1,2,3,4,5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep_a_ids = [get_kept_article_ids_chunkl('validation',p) for p in [1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep_a_ids = [get_kept_article_ids_chunkl('test',p) for p in [1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_future_rewards(step_rewards):\n",
    "    return np.flip(np.cumsum(np.flip(step_rewards)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits = ['train','test','validation']\n",
    "# a_id_len = dict([\n",
    "#     (s,dict()) for s in splits\n",
    "# ])\n",
    "\n",
    "# arts = []\n",
    "# a_top_gold_actions = []\n",
    "# a_top_gold_future_rewards = []\n",
    "# for split in splits:\n",
    "#     if split in ['validation','train']:\n",
    "#         continue\n",
    "#     else:\n",
    "#         for (a_id, a_s_idx_list, a_gold_acts_list, a_gold_reward_list) in zip(\n",
    "#             dataset['id'], dataset['article_top_sent'],\n",
    "#             dataset['all_gold_actions'], dataset['all_gold_rewards']\n",
    "#         ):\n",
    "#             a_len = len(a_s_idx_list)\n",
    "#             if a_len!=10:\n",
    "#                 continue\n",
    "#             arts.append(a_id)\n",
    "#             a_gold_reward_list = np.array(a_gold_reward_list)\n",
    "#             best_summary_index = np.argmax(\n",
    "#                 a_gold_reward_list[\n",
    "#                     np.arange(a_gold_reward_list.shape[0]),\n",
    "#                     (a_gold_reward_list!=0).cumsum(1).argmax(1)\n",
    "#                 ]\n",
    "#             )\n",
    "#             a_top_gold_actions.append(a_gold_acts_list[best_summary_index])\n",
    "#             a_top_gold_future_rewards.append(get_future_rewards(a_gold_reward_list[best_summary_index]))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits = ['train','test','validation']\n",
    "# a_id_len = dict([\n",
    "#     (s,dict()) for s in splits\n",
    "# ])\n",
    "\n",
    "# arts = []\n",
    "# a_top_gold_actions = []\n",
    "# a_top_gold_future_rewards = []\n",
    "# for split in splits:\n",
    "#     if split in ['test','train']:\n",
    "#         continue\n",
    "#     else:\n",
    "#         for (a_id, a_s_idx_list, a_gold_acts_list, a_gold_reward_list) in zip(\n",
    "#             dataset['id'], dataset['article_top_sent'],\n",
    "#             dataset['all_gold_actions'], dataset['all_gold_rewards']\n",
    "#         ):\n",
    "#             a_len = len(a_s_idx_list)\n",
    "#             if a_len!=10:\n",
    "#                 continue\n",
    "#             arts.append(a_id)\n",
    "#             a_gold_reward_list = np.array(a_gold_reward_list)\n",
    "#             best_summary_index = np.argmax(\n",
    "#                 a_gold_reward_list[\n",
    "#                     np.arange(a_gold_reward_list.shape[0]),\n",
    "#                     (a_gold_reward_list!=0).cumsum(1).argmax(1)\n",
    "#                 ]\n",
    "#             )\n",
    "#             a_top_gold_actions.append(a_gold_acts_list[best_summary_index])\n",
    "#             a_top_gold_future_rewards.append(get_future_rewards(a_gold_reward_list[best_summary_index]))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = ['train','test','validation']\n",
    "a_id_len = dict([\n",
    "    (s,dict()) for s in splits\n",
    "])\n",
    "\n",
    "arts = []\n",
    "a_top_gold_actions = []\n",
    "a_top_gold_future_rewards = []\n",
    "for split in splits:\n",
    "    if split in ['test','validation']:\n",
    "        continue\n",
    "    else:\n",
    "        for (a_id, a_s_idx_list, a_gold_acts_list, a_gold_reward_list) in zip(\n",
    "            dataset['article_id']['article_id'], dataset['top_sentences_index']['top_sentences_index'],\n",
    "            dataset['all_gold_actions']['all_gold_actions'], dataset['all_gold_rewards']['all_gold_rewards']\n",
    "        ):\n",
    "            a_len = len(a_s_idx_list)\n",
    "            if a_len!=10:\n",
    "                continue\n",
    "            arts.append(a_id)\n",
    "            a_gold_reward_list = np.array(a_gold_reward_list)\n",
    "            best_summary_index = np.argmax(\n",
    "                a_gold_reward_list[\n",
    "                    np.arange(a_gold_reward_list.shape[0]),\n",
    "                    (a_gold_reward_list!=0).cumsum(1).argmax(1)\n",
    "                ]\n",
    "            )\n",
    "            a_top_gold_actions.append(a_gold_acts_list[best_summary_index])\n",
    "            a_top_gold_future_rewards.append(get_future_rewards(a_gold_reward_list[best_summary_index]))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11251, 11251)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(arts), sum([len(k) for k in keep_a_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created chunk: 1 - indices 0 - 11251\n"
     ]
    }
   ],
   "source": [
    "# start = 0\n",
    "# for i,a_ids in enumerate(keep_a_ids):\n",
    "#     part = i+1\n",
    "#     end = start+len(a_ids)\n",
    "    \n",
    "#     gold_action_chunk = a_top_gold_actions[start:end]\n",
    "#     gold_rewards_chunk = a_top_gold_future_rewards[start:end]\n",
    "#     with open(\n",
    "#     top_sentence_emb_path+'/test/'+'test_gold_actions_part_'+str(part)+'.pkl', \n",
    "#         'wb') as pickle_file:\n",
    "#         pickle.dump(gold_action_chunk, pickle_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "#     with open(\n",
    "#         top_sentence_emb_path+'/test/'+'test_gold_future_rewards_part_'+str(part)+'.pkl', \n",
    "#         'wb') as pickle_file:\n",
    "#         pickle.dump(gold_rewards_chunk, pickle_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "#     print('Created chunk:',part,'- indices',start,'-',end)\n",
    "#     start = end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created chunk: 1 - indices 0 - 13111\n"
     ]
    }
   ],
   "source": [
    "# start = 0\n",
    "# for i,a_ids in enumerate(keep_a_ids):\n",
    "#     part = i+1\n",
    "#     end = start+len(a_ids)\n",
    "    \n",
    "#     gold_action_chunk = a_top_gold_actions[start:end]\n",
    "#     gold_rewards_chunk = a_top_gold_future_rewards[start:end]\n",
    "#     with open(\n",
    "#     top_sentence_emb_path+'/validation/'+'validation_gold_actions_part_'+str(part)+'.pkl', \n",
    "#         'wb') as pickle_file:\n",
    "#         pickle.dump(gold_action_chunk, pickle_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "#     with open(\n",
    "#         top_sentence_emb_path+'/validation/'+'validation_gold_future_rewards_part_'+str(part)+'.pkl', \n",
    "#         'wb') as pickle_file:\n",
    "#         pickle.dump(gold_rewards_chunk, pickle_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "#     print('Created chunk:',part,'- indices',start,'-',end)\n",
    "#     start = end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created chunk: 1 - indices 0 - 60000\n",
      "Created chunk: 2 - indices 60000 - 120000\n",
      "Created chunk: 3 - indices 120000 - 180000\n",
      "Created chunk: 4 - indices 180000 - 240000\n",
      "Created chunk: 5 - indices 240000 - 282638\n"
     ]
    }
   ],
   "source": [
    "start = 0\n",
    "for i,a_ids in enumerate(keep_a_ids):\n",
    "    part = i+1\n",
    "    end = start+len(a_ids)\n",
    "    \n",
    "    gold_action_chunk = a_top_gold_actions[start:end]\n",
    "    gold_rewards_chunk = a_top_gold_future_rewards[start:end]\n",
    "    with open(\n",
    "    top_sentence_emb_path+'/train/'+'train_gold_actions_part_'+str(part)+'.pkl', \n",
    "        'wb') as pickle_file:\n",
    "        pickle.dump(gold_action_chunk, pickle_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    with open(\n",
    "        top_sentence_emb_path+'/train/'+'train_gold_future_rewards_part_'+str(part)+'.pkl', \n",
    "        'wb') as pickle_file:\n",
    "        pickle.dump(gold_rewards_chunk, pickle_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    print('Created chunk:',part,'- indices',start,'-',end)\n",
    "    start = end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "1b_retrieve_data_sentences.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
