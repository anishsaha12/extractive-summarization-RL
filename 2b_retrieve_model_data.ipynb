{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "lSe7nuS9SHHI"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import torch\n",
    "import pickle\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2YeGUkbxSpfx"
   },
   "outputs": [],
   "source": [
    "data_base = 'data'\n",
    "dataset_name = 'cnn_dailymail'\n",
    "\n",
    "data_path = data_base+'/'+dataset_name\n",
    "sentence_emb_path = data_path+'/sentence_embs'\n",
    "top_sentence_emb_path = data_path+'/top_sentence_embs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_from_disk(\n",
    "#     'data/cnn_dailymail/sentences/test/test_rewards'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_from_disk(\n",
    "#     'data/cnn_dailymail/sentences/validation/val_rewards'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk(\n",
    "    'data/cnn_dailymail/sentences/train/train_rewards'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = ['train','test','validation']\n",
    "a_id_len = dict([\n",
    "    (s,dict()) for s in splits\n",
    "])\n",
    "\n",
    "for split in splits:\n",
    "    if split in ['test']:\n",
    "        continue\n",
    "    else:\n",
    "        cum_sum = 0\n",
    "        for a_id, a_s_idx_list in zip(dataset['article_id']['article_id'], dataset['top_sentences_index']['top_sentences_index']):\n",
    "            a_len = len(a_s_idx_list)\n",
    "            a_id_len[split][a_id] = (a_len, cum_sum)\n",
    "            cum_sum += a_len\n",
    "with open(top_sentence_emb_path+'/articleID_sentences_info.pkl', 'wb') as pickle_file:\n",
    "    pickle.dump(a_id_len, pickle_file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits = ['train','test','validation']\n",
    "\n",
    "# for split in splits:\n",
    "#     if split in ['test', 'train']:\n",
    "#         continue\n",
    "#     else:\n",
    "#         cum_sum = 0\n",
    "#         for a_id, a_s_idx_list in zip(dataset['id'], dataset['article_top_sent']):\n",
    "#             a_len = len(a_s_idx_list)\n",
    "#             a_id_len[split][a_id] = (a_len, cum_sum)\n",
    "#             cum_sum += a_len\n",
    "# with open(top_sentence_emb_path+'/articleID_sentences_info.pkl', 'wb') as pickle_file:\n",
    "#     pickle.dump(a_id_len, pickle_file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits = ['train','test','validation']\n",
    "\n",
    "# for split in splits:\n",
    "#     if split in ['validation', 'train']:\n",
    "#         continue\n",
    "#     else:\n",
    "#         cum_sum = 0\n",
    "#         for a_id, a_s_idx_list in zip(dataset['id'], dataset['article_top_sent']):\n",
    "#             a_len = len(a_s_idx_list)\n",
    "#             a_id_len[split][a_id] = (a_len, cum_sum)\n",
    "#             cum_sum += a_len\n",
    "# with open(top_sentence_emb_path+'/articleID_sentences_info.pkl', 'wb') as pickle_file:\n",
    "#     pickle.dump(a_id_len, pickle_file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(top_sentence_emb_path+'/articleID_sentences_info.pkl', \"rb\") as input_f:\n",
    "    a_id_len = pickle.load(input_f)\n",
    "def get_chunk_sent_idx(split, a_id, idx):\n",
    "    num_s_per_chunk = 600000\n",
    "    s_idx = a_id_len[split][a_id][1]+idx\n",
    "    chunk_num = int(s_idx/num_s_per_chunk)+1\n",
    "    chunk_s_idx = int((s_idx%num_s_per_chunk))\n",
    "    return chunk_num, chunk_s_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = ['train','test','validation']\n",
    "required_items = dict([\n",
    "    (s,[]) for s in splits\n",
    "])\n",
    "for split in splits:\n",
    "    if split in ['test','validation']:\n",
    "        continue\n",
    "    for a_id, a_s_idx_list in zip(dataset['article_id']['article_id'], dataset['top_sentences_index']['top_sentences_index']):\n",
    "        article_items = []\n",
    "        for idx, a_s_idx in enumerate(a_s_idx_list):\n",
    "            chunk_num, s_idx = get_chunk_sent_idx('train', a_id, idx)\n",
    "            article_items.append((chunk_num, s_idx))\n",
    "        required_items[split].append((a_id, len(a_s_idx_list), article_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits = ['train','test','validation']\n",
    "\n",
    "# for split in splits:\n",
    "#     if split in ['test','train']:\n",
    "#         continue\n",
    "#     for a_id, a_s_idx_list in zip(dataset['id'], dataset['article_top_sent']):\n",
    "#         article_items = []\n",
    "#         for idx, a_s_idx in enumerate(a_s_idx_list):\n",
    "#             chunk_num, s_idx = get_chunk_sent_idx('validation', a_id, idx)\n",
    "#             article_items.append((chunk_num, s_idx))\n",
    "#         required_items[split].append((a_id, len(a_s_idx_list), article_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splits = ['train','test','validation']\n",
    "\n",
    "# for split in splits:\n",
    "#     if split in ['validation','train']:\n",
    "#         continue\n",
    "#     for a_id, a_s_idx_list in zip(dataset['id'], dataset['article_top_sent']):\n",
    "#         article_items = []\n",
    "#         for idx, a_s_idx in enumerate(a_s_idx_list):\n",
    "#             chunk_num, s_idx = get_chunk_sent_idx('test', a_id, idx)\n",
    "#             article_items.append((chunk_num, s_idx))\n",
    "#         required_items[split].append((a_id, len(a_s_idx_list), article_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(top_sentence_emb_path+'/articleID_len_sents_chunk_info.pkl', 'wb') as pickle_file:\n",
    "    pickle.dump(required_items, pickle_file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(287113,\n",
       " ('ffffd563a96104f5cf4493cfa701a65f31b06abf',\n",
       "  10,\n",
       "  [(5, 460792),\n",
       "   (5, 460793),\n",
       "   (5, 460794),\n",
       "   (5, 460795),\n",
       "   (5, 460796),\n",
       "   (5, 460797),\n",
       "   (5, 460798),\n",
       "   (5, 460799),\n",
       "   (5, 460800),\n",
       "   (5, 460801)]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(required_items['train']), required_items['train'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(top_sentence_emb_path+'/articleID_len_sents_chunk_info.pkl', \"rb\") as input_f:\n",
    "    required_items = pickle.load(input_f)\n",
    "def get_article_sent_emb(split, chunk_num, s_idx, sent_embs_chunk=None):\n",
    "    if sent_embs_chunk==None:\n",
    "        with open(\n",
    "          top_sentence_emb_path+'/'+split+'/'+split+'_top_sentences_part_'+str(chunk_num)+\".pkl\", \n",
    "          \"rb\"\n",
    "        ) as input_f:\n",
    "            sent_embs_chunk = pickle.load(input_f)\n",
    "    return sent_embs_chunk[s_idx], sent_embs_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_current_chunk = None\n",
    "sent_embs_chunk = None\n",
    "res = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_articles = []\n",
    "for i,(a_id, a_len, chunk_sents) in enumerate(required_items['train']):\n",
    "    if i<=(244139-1):\n",
    "        continue\n",
    "    else:\n",
    "        if len(keep_articles)==60000:\n",
    "            break\n",
    "        if a_len!=10:\n",
    "            continue\n",
    "        keep_articles.append(a_id)\n",
    "        article_res = []\n",
    "        for (chunk_num, s_idx) in chunk_sents:\n",
    "            if loaded_current_chunk==chunk_num:\n",
    "                s_emb, sent_embs_chunk = get_article_sent_emb(\n",
    "                    'train', chunk_num, s_idx, sent_embs_chunk\n",
    "                )\n",
    "            else:\n",
    "                print('  Loading chunk:', chunk_num)\n",
    "                loaded_current_chunk = chunk_num\n",
    "                s_emb, sent_embs_chunk = get_article_sent_emb(\n",
    "                    'train', chunk_num, s_idx, None\n",
    "                )\n",
    "            article_res.append(s_emb)\n",
    "        res.append(article_res)\n",
    "        if i%500==0:\n",
    "            print('Loaded',i+1,'articles of',len(required_items['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loading chunk: 1\n",
      "Loaded 1 articles of 13368\n",
      "Loaded 501 articles of 13368\n",
      "Loaded 1001 articles of 13368\n",
      "Loaded 1501 articles of 13368\n",
      "Loaded 2001 articles of 13368\n",
      "Loaded 2501 articles of 13368\n",
      "Loaded 3001 articles of 13368\n",
      "Loaded 3501 articles of 13368\n",
      "Loaded 4001 articles of 13368\n",
      "Loaded 4501 articles of 13368\n",
      "Loaded 5001 articles of 13368\n",
      "Loaded 5501 articles of 13368\n",
      "Loaded 6001 articles of 13368\n",
      "Loaded 6501 articles of 13368\n",
      "Loaded 7001 articles of 13368\n",
      "Loaded 8001 articles of 13368\n",
      "Loaded 8501 articles of 13368\n",
      "Loaded 9001 articles of 13368\n",
      "Loaded 9501 articles of 13368\n",
      "Loaded 10001 articles of 13368\n",
      "Loaded 11001 articles of 13368\n",
      "Loaded 11501 articles of 13368\n",
      "Loaded 12001 articles of 13368\n",
      "Loaded 12501 articles of 13368\n",
      "Loaded 13001 articles of 13368\n"
     ]
    }
   ],
   "source": [
    "# keep_articles = []\n",
    "# for i,(a_id, a_len, chunk_sents) in enumerate(required_items['validation']):\n",
    "# #     if i<=(244139-1):\n",
    "# #         continue\n",
    "# #     else:\n",
    "#         if len(keep_articles)==60000:\n",
    "#             break\n",
    "#         if a_len!=10:\n",
    "#             continue\n",
    "#         keep_articles.append(a_id)\n",
    "#         article_res = []\n",
    "#         for (chunk_num, s_idx) in chunk_sents:\n",
    "#             if loaded_current_chunk==chunk_num:\n",
    "#                 s_emb, sent_embs_chunk = get_article_sent_emb(\n",
    "#                     'validation', chunk_num, s_idx, sent_embs_chunk\n",
    "#                 )\n",
    "#             else:\n",
    "#                 print('  Loading chunk:', chunk_num)\n",
    "#                 loaded_current_chunk = chunk_num\n",
    "#                 s_emb, sent_embs_chunk = get_article_sent_emb(\n",
    "#                     'validation', chunk_num, s_idx, None\n",
    "#                 )\n",
    "#             article_res.append(s_emb)\n",
    "#         res.append(article_res)\n",
    "#         if i%500==0:\n",
    "#             print('Loaded',i+1,'articles of',len(required_items['validation']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Loading chunk: 1\n",
      "Loaded 1 articles of 11490\n",
      "Loaded 501 articles of 11490\n",
      "Loaded 1001 articles of 11490\n",
      "Loaded 1501 articles of 11490\n",
      "Loaded 2001 articles of 11490\n",
      "Loaded 2501 articles of 11490\n",
      "Loaded 3001 articles of 11490\n",
      "Loaded 3501 articles of 11490\n",
      "Loaded 4001 articles of 11490\n",
      "Loaded 4501 articles of 11490\n",
      "Loaded 5001 articles of 11490\n",
      "Loaded 5501 articles of 11490\n",
      "Loaded 6001 articles of 11490\n",
      "Loaded 6501 articles of 11490\n",
      "Loaded 7001 articles of 11490\n",
      "Loaded 7501 articles of 11490\n",
      "Loaded 8001 articles of 11490\n",
      "Loaded 8501 articles of 11490\n",
      "Loaded 9001 articles of 11490\n",
      "Loaded 9501 articles of 11490\n",
      "Loaded 10001 articles of 11490\n",
      "Loaded 10501 articles of 11490\n",
      "Loaded 11001 articles of 11490\n"
     ]
    }
   ],
   "source": [
    "# keep_articles = []\n",
    "# for i,(a_id, a_len, chunk_sents) in enumerate(required_items['test']):\n",
    "# #     if i<=(244139-1):\n",
    "# #         continue\n",
    "# #     else:\n",
    "#         if len(keep_articles)==60000:\n",
    "#             break\n",
    "#         if a_len!=10:\n",
    "#             continue\n",
    "#         keep_articles.append(a_id)\n",
    "#         article_res = []\n",
    "#         for (chunk_num, s_idx) in chunk_sents:\n",
    "#             if loaded_current_chunk==chunk_num:\n",
    "#                 s_emb, sent_embs_chunk = get_article_sent_emb(\n",
    "#                     'test', chunk_num, s_idx, sent_embs_chunk\n",
    "#                 )\n",
    "#             else:\n",
    "#                 print('  Loading chunk:', chunk_num)\n",
    "#                 loaded_current_chunk = chunk_num\n",
    "#                 s_emb, sent_embs_chunk = get_article_sent_emb(\n",
    "#                     'test', chunk_num, s_idx, None\n",
    "#                 )\n",
    "#             article_res.append(s_emb)\n",
    "#         res.append(article_res)\n",
    "#         if i%500==0:\n",
    "#             print('Loaded',i+1,'articles of',len(required_items['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11251, 11489, 'fffd506034c5275fe57220e669ad7e01605d597c')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res), i, keep_articles[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# part = 1\n",
    "# with open(\n",
    "#     top_sentence_emb_path+'/validation/'+'validation_data_part_'+str(part)+'.pkl', \n",
    "#     'wb') as pickle_file:\n",
    "#     pickle.dump(res, pickle_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# with open(\n",
    "#     top_sentence_emb_path+'/validation/'+'validation_data_article_ids_part_'+str(part)+'.pkl', \n",
    "#     'wb') as pickle_file:\n",
    "#     pickle.dump(keep_articles, pickle_file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# part = 1\n",
    "# with open(\n",
    "#     top_sentence_emb_path+'/test/'+'test_data_part_'+str(part)+'.pkl', \n",
    "#     'wb') as pickle_file:\n",
    "#     pickle.dump(res, pickle_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# with open(\n",
    "#     top_sentence_emb_path+'/test/'+'test_data_article_ids_part_'+str(part)+'.pkl', \n",
    "#     'wb') as pickle_file:\n",
    "#     pickle.dump(keep_articles, pickle_file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "part = 1\n",
    "with open(\n",
    "    top_sentence_emb_path+'/train/'+'train_data_part_'+str(part)+'.pkl', \n",
    "    'wb') as pickle_file:\n",
    "    pickle.dump(res, pickle_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(\n",
    "    top_sentence_emb_path+'/train/'+'train_data_article_ids_part_'+str(part)+'.pkl', \n",
    "    'wb') as pickle_file:\n",
    "    pickle.dump(keep_articles, pickle_file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "part = 2\n",
    "with open(\n",
    "    top_sentence_emb_path+'/train/'+'train_data_part_'+str(part)+'.pkl', \n",
    "    'wb') as pickle_file:\n",
    "    pickle.dump(res, pickle_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(\n",
    "    top_sentence_emb_path+'/train/'+'train_data_article_ids_part_'+str(part)+'.pkl', \n",
    "    'wb') as pickle_file:\n",
    "    pickle.dump(keep_articles, pickle_file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "part = 3\n",
    "with open(\n",
    "    top_sentence_emb_path+'/train/'+'train_data_part_'+str(part)+'.pkl', \n",
    "    'wb') as pickle_file:\n",
    "    pickle.dump(res, pickle_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(\n",
    "    top_sentence_emb_path+'/train/'+'train_data_article_ids_part_'+str(part)+'.pkl', \n",
    "    'wb') as pickle_file:\n",
    "    pickle.dump(keep_articles, pickle_file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "part = 4\n",
    "with open(\n",
    "    top_sentence_emb_path+'/train/'+'train_data_part_'+str(part)+'.pkl', \n",
    "    'wb') as pickle_file:\n",
    "    pickle.dump(res, pickle_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(\n",
    "    top_sentence_emb_path+'/train/'+'train_data_article_ids_part_'+str(part)+'.pkl', \n",
    "    'wb') as pickle_file:\n",
    "    pickle.dump(keep_articles, pickle_file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "part = 5\n",
    "with open(\n",
    "    top_sentence_emb_path+'/train/'+'train_data_part_'+str(part)+'.pkl', \n",
    "    'wb') as pickle_file:\n",
    "    pickle.dump(res, pickle_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open(\n",
    "    top_sentence_emb_path+'/train/'+'train_data_article_ids_part_'+str(part)+'.pkl', \n",
    "    'wb') as pickle_file:\n",
    "    pickle.dump(keep_articles, pickle_file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "1b_retrieve_data_sentences.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
